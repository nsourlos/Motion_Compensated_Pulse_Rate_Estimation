{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.signal\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "def BandpassFilter(signal, pass_band):\n",
    "    \"\"\"\n",
    "    Takes the signal and a pass band as inputs\n",
    "\n",
    "    Returns:\n",
    "        Filter signal that zeros all frequencies outside the pass band.\n",
    "    \"\"\"\n",
    "    b, a = sp.signal.butter(5, pass_band, btype='bandpass', fs=125)\n",
    "    return sp.signal.filtfilt(b, a, signal)\n",
    "    \n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        # Compute aggregate error metric\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "def four(sig,fs):\n",
    "    \"\"\"\n",
    "    Takes the signal and a frequency as inputs\n",
    "\n",
    "    Returns:\n",
    "        The magnitude of the fourier transform along with the frequencies of the fourier domain\n",
    "    \"\"\"\n",
    "    freqs=np.fft.rfftfreq(len(sig),1/fs)\n",
    "    fft_mag=np.abs(np.fft.rfft(sig))\n",
    "    return freqs,fft_mag\n",
    "\n",
    "def LowpassFilter(signal, fs):\n",
    "    \"\"\"\n",
    "    Takes the signal and a frequency as inputs\n",
    "\n",
    "    Returns:\n",
    "     Filter signal that zeros all frequencies higher than the specified frequency.\n",
    "    \"\"\"\n",
    "    b, a = sp.signal.butter(3, 12, btype='lowpass', fs=fs)\n",
    "    return sp.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def Featurize(accx, accy, accz, fs):\n",
    "    \"\"\"A partial featurization of the accelerometer signal.\n",
    "    \n",
    "    Args:\n",
    "        accx: (np.array) x-channel of the accelerometer.\n",
    "        accy: (np.array) y-channel of the accelerometer.\n",
    "        accz: (np.array) z-channel of the accelerometer.\n",
    "        fs: (number) the sampling rate of the accelerometer\n",
    "        \n",
    "    Returns:\n",
    "        n-tuple of accelerometer features\n",
    "    \"\"\"\n",
    "    \n",
    "    accx = LowpassFilter(accx, fs)\n",
    "    accy = LowpassFilter(accy, fs)\n",
    "    accz = LowpassFilter(accz, fs)\n",
    "    \n",
    "    # The mean of the x-channel\n",
    "    mn_x = np.mean(accx)\n",
    "\n",
    "    # The standard deviation of the x-channel\n",
    "    std_x = np.std(accx)\n",
    "\n",
    "    # The 5th percentile of the x-channel\n",
    "    p5_x = np.percentile(accx, 5)\n",
    "\n",
    "    # The pearson correlation coefficient between the x and y channels\n",
    "    corr_xy = sp.stats.pearsonr(accx, accy)[0]\n",
    "\n",
    "    # The total AC energy of the x-axis\n",
    "    energy_x = np.sum(np.square(accx - np.mean(accx)))  # np.var(accx) * len(accx)\n",
    "    \n",
    "    # Take an FFT of the signal. If the signal is too short, 0-pad it so we have at least 2046 points in the FFT.\n",
    "    fft_len = len(accx)#max(len(accx), 2046)\n",
    "    \n",
    "    # Create an array of frequency bins\n",
    "    freqs = np.fft.rfftfreq(fft_len, 1 / fs)\n",
    "    \n",
    "    # Take an FFT of the centered signal\n",
    "    fft_x = np.fft.rfft(accx - np.mean(accx), fft_len)\n",
    "    \n",
    "    # The frequency with the most power between 0.25 and 10 Hz\n",
    "    low_freqs = (freqs >= 0.25) & (freqs <= 10)\n",
    "    dominant_frequency_x = freqs[low_freqs][np.argmax(np.abs(fft_x)[low_freqs])]\n",
    "\n",
    "    # The fraction of energy between 0.25 and 10 Hz in the x-channel\n",
    "    spectral_energy_x = np.square(np.abs(fft_x))\n",
    "    energy_23_x = (np.sum(spectral_energy_x[(freqs >= 0.25) & (freqs <= 10)])\n",
    "                   / np.sum(spectral_energy_x))\n",
    "    \n",
    "    return (mn_x,\n",
    "            std_x,\n",
    "            p5_x,\n",
    "            corr_xy,\n",
    "            energy_x,\n",
    "            dominant_frequency_x,\n",
    "            energy_23_x)\n",
    "\n",
    "def Featurize2(acc, fs):\n",
    "    \"\"\"A partial featurization of the accelerometer signal.\n",
    "    \n",
    "    Args:\n",
    "        accx: (np.array) x-channel of the accelerometer.\n",
    "        accy: (np.array) y-channel of the accelerometer.\n",
    "        accz: (np.array) z-channel of the accelerometer.\n",
    "        fs: (number) the sampling rate of the accelerometer\n",
    "        \n",
    "    Returns:\n",
    "        n-tuple of accelerometer features\n",
    "    \"\"\"\n",
    "    \n",
    "#     acc = LowpassFilter(acc, fs)\n",
    "    \n",
    "    # The mean of the x-channel\n",
    "    mn = np.mean(acc)\n",
    "\n",
    "    # The standard deviation of the x-channel\n",
    "    std = np.std(acc)\n",
    "\n",
    "    # The 5th percentile of the x-channel\n",
    "    p5 = np.percentile(acc, 5)\n",
    "\n",
    "#     # The pearson correlation coefficient between the x and y channels\n",
    "#     corr_xy = sp.stats.pearsonr(accx, accy)[0]\n",
    "\n",
    "    # The total AC energy of the x-axis\n",
    "    energy = np.sum(np.square(acc - np.mean(acc)))  # np.var(accx) * len(accx)\n",
    "    \n",
    "    # Take an FFT of the signal. If the signal is too short, 0-pad it so we have at least 2046 points in the FFT.\n",
    "    fft_len = len(acc)#max(len(acc)), 2046)\n",
    "    \n",
    "    # Create an array of frequency bins\n",
    "    freqs = np.fft.rfftfreq(fft_len, 1 / fs)\n",
    "    \n",
    "    # Take an FFT of the centered signal\n",
    "    ffta = np.fft.rfft(acc - np.mean(acc), fft_len)\n",
    "    \n",
    "    # The frequency with the most power between 0.25 and 10 Hz\n",
    "    low_freqs = (freqs >= 0.25) & (freqs <= 10)\n",
    "    dominant_frequency = freqs[low_freqs][np.argmax(np.abs(ffta)[low_freqs])]\n",
    "\n",
    "    # The fraction of energy between 0.25 and 10 Hz#### in the x-channel\n",
    "    spectral_energy = np.square(np.abs(ffta))\n",
    "    energy_23 = (np.sum(spectral_energy[(freqs >= 0.25) & (freqs <= 10)])\n",
    "                   / np.sum(spectral_energy))\n",
    "    \n",
    "    return (mn,\n",
    "            std,\n",
    "            p5,\n",
    "#             corr_xy,\n",
    "            energy,\n",
    "            dominant_frequency,\n",
    "            energy_23)\n",
    "\n",
    "# def plot_fft(signal, freqs, fft, fs, plotype):\n",
    "#     \"\"\"A function to plot the frequencies of a signal in the fourier domain.\n",
    "    \n",
    "#     Args:\n",
    "#         signal: The original signal.\n",
    "#         freqs: The frequencies of the signal in the fourier domain.\n",
    "#         fft: The fourier transform of the signal.\n",
    "#         fs: (number) the sampling rate frequency of the signal\n",
    "#         plotype:\n",
    "        \n",
    "#     Returns:\n",
    "#         n-tuple of accelerometer features\n",
    "#     \"\"\"\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.subplot(2,1,1)\n",
    "#     ts = np.arange(len(signal)) / fs\n",
    "#     plt.plot(ts, signal)\n",
    "#     plt.subplot(2,1,2)\n",
    "#     plt.plot(freqs, np.abs(fft))\n",
    "#     plt.title(\"freq domain of {}\".format(plotype))\n",
    "#     plt.xlabel(\"freq\")\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "def confidencefun(freqs, fft_mag, bpm_f):\n",
    "    '''\n",
    "    Function that gets the frequencies and the magnitude of the fourier transform of the signal in a window\n",
    "    along with an estimated BPM frequency (in Hz) and returns a confidence interval for that frequency\n",
    "    '''\n",
    "    window_f = 0.2 #30/60\n",
    "    fundamental_freq_window = (freqs > bpm_f - window_f) & (freqs < bpm_f + window_f)\n",
    "    return np.sum(fft_mag[fundamental_freq_window])/ np.sum(fft_mag)\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):#,activ,sub):\n",
    "    \"\"\"\n",
    "    Takes the magnitude of the accelerations in x y and z axis, applies a bandpass filter\n",
    "    to them and to the PPG signal, and for each window of size 8 sec (next window after 2 sec),\n",
    "    gets the fourier transform of the magnitude of acceleration and of PPG signal, finds the peak\n",
    "    values, compares them and if they are different they keep that dominant frequency and if not \n",
    "    they calculate the next 4 until it finds the dominant.\n",
    "    \n",
    "    Args:\n",
    "        data_fl: file with the original data.\n",
    "        ref_fl: file with the ground truth labels.\n",
    "        \n",
    "    Returns:\n",
    "        errors for each dominant frequency along with its confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    fs=125\n",
    "    # Load data using LoadTroikaDataFile\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl) \n",
    "\n",
    "    #Load ground truth labels\n",
    "    labelsnew = sp.io.loadmat(ref_fl)['BPM0'] #As shown in keys of dictionary\n",
    "#     print(labelsnew.shape)\n",
    "\n",
    "    #Get magnitude of accelaration\n",
    "    magn=np.sqrt(np.square(accx)+np.square(accy)+np.square(accz))\n",
    "\n",
    "    # Bandpass between 60-180BPM since it was found that minimum BPM is 66 and maximum is 178 in all subjects\n",
    "    low=60/60\n",
    "    high=240/60 #had 180/60, changed after feedback\n",
    "    pass_band=(low,high)\n",
    "    fil_ppg = BandpassFilter(ppg, pass_band)\n",
    "    fil_mag = BandpassFilter(magn, pass_band)\n",
    "#     frequenc,magnit=four(fil_ppg,125)\n",
    "#     plt.plot(frequenc,magnit)\n",
    "    \n",
    "#     #Spectrograms - BPM every 8 sec window and windows overlap every 6 secs\n",
    "#     #Frequency does not change significantly over time\n",
    "#     plt.specgram(fil_ppg, Fs=fs) #NFFT=8*fs,noverlap=6*fs,  xextent=((0, 300))     \n",
    "#     plt.ylim((0, 10))\n",
    "\n",
    "#     p_spec,p_freq,_,_=plt.specgram(fil_ppg, Fs=fs, NFFT=8*fs, noverlap=6*fs, xextent=((0, 300)))\n",
    "#     a_spec,a_freq,_,_=plt.specgram(fil_mag, Fs=fs, NFFT=8*fs, noverlap=6*fs, xextent=((0, 300)))\n",
    "#     print(p_spec.shape)\n",
    "#     print(p_freq.shape)\n",
    "#     print(a_spec.shape)\n",
    "#     print(a_freq.shape)\n",
    "   \n",
    "    \n",
    "    # Compute pulse rate estimates and estimation confidence.\n",
    "    #Feature extraction\n",
    "    #Window length is 8 and windows overlap for 6 secs so shift=2sec, based on documentation\n",
    "    window_length_s = 8\n",
    "    window_shift_s = 2\n",
    "    window_length = window_length_s * fs\n",
    "    window_shift = window_shift_s * fs\n",
    "    labels, subjects, features = [], [], []\n",
    "    errors,confidence=[], []\n",
    "    est_bpm=[]\n",
    "#     prev_est=0\n",
    "    ind=-1\n",
    "\n",
    "    for i in range(0, len(ppg) - window_length, window_shift):\n",
    "        ind=ind+1\n",
    "        mag_new=fil_mag[i: i + window_length]\n",
    "        ppg_new=fil_ppg[i: i + window_length]\n",
    "        \n",
    "        #Fourier\n",
    "        freqs_ac, mags_ac = four(mag_new, fs)\n",
    "        freqs_p, mags_p = four(ppg_new, fs)\n",
    "#         plt.plot(freqs_p,mags_p)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Find peaks\n",
    "        ppg_peaks = sp.signal.find_peaks(mags_p)[0] #, height=2000\n",
    "#         ppg_peaks_f = freqs_p[ppg_peaks]\n",
    "        ppg_peaks_f = mags_p[ppg_peaks]\n",
    "        \n",
    "#         plt.plot(mags_p)\n",
    "#         plt.plot(ppg_peaks,mags_p[ppg_peaks],'b.',ms=0.1) #\n",
    "        \n",
    "        acc_peaks = sp.signal.find_peaks(mags_ac)[0] #, height=None\n",
    "#         acc_peaks_f = freqs_ac[acc_peaks]\n",
    "        acc_peaks_f = mags_ac[acc_peaks]\n",
    "\n",
    "#         plt.plot(mags_ac)\n",
    "#         plt.plot(acc_peaks,mags_ac[acc_peaks],'r.',ms=0.1) \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        ppg_max = freqs_p[np.argmax(mags_p)]\n",
    "#         print(ppg_max)\n",
    "#         print(\"\\n\")\n",
    "        acc_max = freqs_ac[np.argmax(mags_ac)]\n",
    "#         print(acc_max)\n",
    "#         print(ppg_max-acc_max)\n",
    "\n",
    "        if (np.abs(ppg_max-acc_max)!=0): #if ac and ppg frequencies are different then this is the pulse for this window\n",
    "            est_bpm.append(ppg_max*60)\n",
    "            est_bpm_f=ppg_max\n",
    "#             print(np.abs(ppg_max-acc_max))\n",
    "        else: #if these frequencies match then\n",
    "                  \n",
    "            k = 1 #find the next 4 candidate frequencies\n",
    "\n",
    "            while np.abs(acc_max-ppg_max) <= 0.2 and k <=4:\n",
    "                k+=1\n",
    "                ppg_max = freqs_p[np.argsort(mags_p, axis=0)[-k]]\n",
    "                acc_max = freqs_ac[np.argsort(mags_ac, axis=0)[-k]]\n",
    "\n",
    "            est_bpm_f = ppg_max\n",
    "\n",
    "#             prev_est = est_bpm_f\n",
    "            est_bpm.append(est_bpm_f*60)\n",
    "    \n",
    "#         est_bpm = [x / 60 for x in est_bpm]\n",
    "        confidence.append(confidencefun(freqs_p, mags_p, est_bpm_f))\n",
    "        \n",
    "        error=np.abs((est_bpm_f*60)-labelsnew[ind])\n",
    "        error=error.tolist()\n",
    "\n",
    "        errors.append(error[0])\n",
    "\n",
    "    return np.asarray(errors), np.asarray(confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppg, accx, accy, accz = LoadTroikaDataFile(LoadTroikaDataset()[0][0])\n",
    "# len(ppg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.03125506467154"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err,conf=RunPulseRateAlgorithm(LoadTroikaDataset()[0][0], LoadTroikaDataset()[1][0])\n",
    "meanabserr=AggregateErrorMetric(err,conf)\n",
    "meanabserr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.870744747297113"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.title('Spectrogram')\n",
    "# plt.specgram(ppg, Fs=fs, NFFT=1000, noverlap=75, xextent=((0, T)))\n",
    "# plt.ylim((0, 20))\n",
    "# plt.xlabel('Time (sec)')\n",
    "# plt.ylabel('Frequency (Hz)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs=125\n",
    "# files=LoadTroikaDataset()\n",
    "# data=files[0]\n",
    "# labels=files[1]\n",
    "# features=[]\n",
    "# labels2=[]\n",
    "# subjects=[]\n",
    "# for i in range(len(data)):\n",
    "#     if data[i][-5:-4]=='1': #type1 activity\n",
    "#         activit=1\n",
    "#     else:\n",
    "#         activit=2\n",
    "# #     print(data[i])    \n",
    "#     feat,lab,subs=RunPulseRateAlgorithm(data[i], labels[i],fs,activ=activit,sub=i)\n",
    "#     features.append(np.array(feat))\n",
    "# #     print(np.array(lab))\n",
    "#     labels2.append(np.array(lab))\n",
    "#     subs=[x+1 for x in subs]\n",
    "#     subjects.append(subs) #+1\n",
    "# features        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(ppg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[0].shape #30 points from 37000+ and 30 datapoints even though 12 subjects since \n",
    "#each 10 second window is its own datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels2 = np.array(labels2)\n",
    "# subjects = np.array(subjects)\n",
    "# features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# n_estimators = 100\n",
    "# max_tree_depth = 4\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "#                              max_depth=max_tree_depth,\n",
    "#                              random_state=42)\n",
    "# clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# class_names = np.array(['class1', 'class2'])\n",
    "# logo = LeaveOneGroupOut()\n",
    "# cm = np.zeros((2, 2), dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_ind, test_ind in logo.split(features, labels2, subjects):\n",
    "#     # For each cross-validation fold...\n",
    "    \n",
    "#     # Split up the dataset into a training and test set.\n",
    "#     # The test set has all the data from just one subject\n",
    "#     X_train, y_train = features[train_ind], labels2[train_ind]\n",
    "#     X_test, y_test = features[test_ind], labels2[test_ind]\n",
    "    \n",
    "#     # Train the classifier\n",
    "#     clf.fit(X_train, y_train)\n",
    "    \n",
    "#     # Run the classifier on the test set\n",
    "#     y_pred = clf.predict(X_test)\n",
    "    \n",
    "#     # Compute the confusion matrix for the test predictions\n",
    "#     c = confusion_matrix(y_test, y_pred, labels=class_names)\n",
    "    \n",
    "#     # Aggregate this confusion matrix with the ones from previous\n",
    "#     # folds.\n",
    "#     cm += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppg, accx, accy, accz = LoadTroikaDataFile(LoadTroikaDataset()[0][1])\n",
    "\n",
    "# fr,ff=four(ppg,125)\n",
    "# plt.plot(fr,ff)\n",
    "# plt.title('Frequency-Domain')\n",
    "# plt.xlabel('Frequency (Hz)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of BPM in all subjects is 66.298\n",
      "Maximum value of BPM in all subjects is 176.7418\n"
     ]
    }
   ],
   "source": [
    "files=LoadTroikaDataset()\n",
    "bpmin=[]\n",
    "bpmax=[]\n",
    "for i in range(len(files[1])):\n",
    "    labelsnew=sp.io.loadmat(files[1][i])\n",
    "    bpmin.append(np.min(labelsnew['BPM0']))\n",
    "    bpmax.append(np.max(labelsnew['BPM0']))\n",
    "print(\"Minimum value of BPM in all subjects is {}\".format(np.min(bpmin)))\n",
    "print(\"Maximum value of BPM in all subjects is {}\".format(np.max(bpmax)))\n",
    "\n",
    "#     print(np.min(labelsnew['BPM0']))\n",
    "#     print(np.max(labelsnew['BPM0']))\n",
    "#     print(\"\\n\")\n",
    "#     print(labelsnew['BPM0'].shape)\n",
    "    \n",
    "    #60-180 in total!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'datasets/troika/training_data/'\n",
    "\n",
    "# file=sp.io.loadmat(data_dir+\"REF_01_TYPE01.mat\")\n",
    "# file['BPM0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'datasets/troika/training_data'\n",
    "# data_dir2='datasets/troika/training_data/'\n",
    "# files=os.listdir(data_dir)\n",
    "# data=[]\n",
    "# labels=[]\n",
    "# for i in range(len(files)):\n",
    "# if 'DATA' in files[i]:\n",
    "#     file=sp.io.loadmat(data_dir2+files[i])\n",
    "#     dat=file['sig'][-3:,:]\n",
    "#     dat=np.transpose(dat)\n",
    "#     subject=files[i][5:7]\n",
    "#     activity=files[i][-5:-4]\n",
    "#     df=pd.DataFrame(data=dat, columns=['accx','accy','accz'])  # 1st row as the column names\n",
    "#     data.append((subject,activity,df))\n",
    "# elif 'REF' in files[i]:\n",
    "#     label=sp.io.loadmat(data_dir2+files[i])\n",
    "#     labels.append(label['BPM0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import activity_classifier_utils\n",
    "\n",
    "# data,ground_truth = activity_classifier_utils.LoadWristPPGDataset()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels, subjects, features = activity_classifier_utils.GenerateFeatures(data,\n",
    "#                                                                         fs,\n",
    "#                                                                         window_length_s=8,\n",
    "#                                                                         window_shift_s=2)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LowpassFilter(signal, fs):\n",
    "#     b, a = sp.signal.butter(3, 12, btype='lowpass', fs=fs)\n",
    "#     return sp.signal.filtfilt(b, a, signal)\n",
    "\n",
    "# def Featurize(accx, accy, accz, fs):\n",
    "#     \"\"\"A partial featurization of the accelerometer signal.\n",
    "    \n",
    "#     Args:\n",
    "#         accx: (np.array) x-channel of the accelerometer.\n",
    "#         accy: (np.array) y-channel of the accelerometer.\n",
    "#         accz: (np.array) z-channel of the accelerometer.\n",
    "#         fs: (number) the sampling rate of the accelerometer\n",
    "        \n",
    "#     Returns:\n",
    "#         n-tuple of accelerometer features\n",
    "#     \"\"\"\n",
    "    \n",
    "#     accx = LowpassFilter(accx, fs)\n",
    "#     accy = LowpassFilter(accy, fs)\n",
    "#     accz = LowpassFilter(accz, fs)\n",
    "    \n",
    "#     # The mean of the x-channel\n",
    "#     mn_x = np.mean(accx)\n",
    "\n",
    "#     # The standard deviation of the x-channel\n",
    "#     std_x = np.std(accx)\n",
    "\n",
    "#     # The 5th percentile of the x-channel\n",
    "#     p5_x = np.percentile(accx, 5)\n",
    "\n",
    "#     # The pearson correlation coefficient between the x and y channels\n",
    "#     corr_xy = sp.stats.pearsonr(accx, accy)[0]\n",
    "\n",
    "#     # The total AC energy of the x-axis\n",
    "#     energy_x = np.sum(np.square(accx - np.mean(accx)))  # np.var(accx) * len(accx)\n",
    "    \n",
    "#     # Take an FFT of the signal. If the signal is too short, 0-pad it so we have at least 2046 points in the FFT.\n",
    "#     fft_len = len(accx)#max(len(accx), 2046)\n",
    "    \n",
    "#     # Create an array of frequency bins\n",
    "#     freqs = np.fft.rfftfreq(fft_len, 1 / fs)\n",
    "    \n",
    "#     # Take an FFT of the centered signal\n",
    "#     fft_x = np.fft.rfft(accx - np.mean(accx), fft_len)\n",
    "    \n",
    "#     # The frequency with the most power between 0.25 and 12 Hz\n",
    "#     low_freqs = (freqs >= 0.25) & (freqs <= 12)\n",
    "#     dominant_frequency_x = freqs[low_freqs][np.argmax(np.abs(fft_x)[low_freqs])]\n",
    "\n",
    "#     # The fraction of energy between 2 and 3 Hz in the x-channel\n",
    "#     spectral_energy_x = np.square(np.abs(fft_x))\n",
    "#     energy_23_x = (np.sum(spectral_energy_x[(freqs >= 2) & (freqs <= 3)])\n",
    "#                    / np.sum(spectral_energy_x))\n",
    "    \n",
    "#     return (mn_x,\n",
    "#             std_x,\n",
    "#             p5_x,\n",
    "#             corr_xy,\n",
    "#             energy_x,\n",
    "#             dominant_frequency_x,\n",
    "#             energy_23_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# fs=125\n",
    "# window_length_s = 8\n",
    "# window_shift_s = 2 #8 sec -6 overlap\n",
    "# window_length = window_length_s * fs\n",
    "# window_shift = window_shift_s * fs\n",
    "# labels, subjects, features = [], [], []\n",
    "# for subject, activity, df in data:\n",
    "#     for i in range(0, len(df) - window_length, window_shift):\n",
    "#         window = df[i: i + window_length]\n",
    "#         accx = window.accx.values\n",
    "#         accy = window.accy.values\n",
    "#         accz = window.accz.values\n",
    "#         features.append(activity_classifier_utils.Featurize(accx, accy, accz, fs=fs))\n",
    "#         labels.append(activity)\n",
    "#         subjects.append(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.array(labels)\n",
    "# subjects = np.array(subjects)\n",
    "# features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# n_estimators = 100\n",
    "# max_tree_depth = 4\n",
    "# clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "#                              max_depth=max_tree_depth,\n",
    "#                              random_state=42)\n",
    "# clf.fit(features, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# class_names = np.array(['01', '02'])\n",
    "# logo = LeaveOneGroupOut()\n",
    "# cm = np.zeros((2, 2), dtype='int')\n",
    "# for train_ind, test_ind in logo.split(features, labels, subjects):\n",
    "#     # For each cross-validation fold...\n",
    "    \n",
    "#     # Split up the dataset into a training and test set.\n",
    "#     # The test set has all the data from just one subject\n",
    "#     X_train, y_train = features[train_ind], labels[train_ind]\n",
    "#     X_test, y_test = features[test_ind], labels[test_ind]\n",
    "    \n",
    "#     # Train the classifier\n",
    "#     clf.fit(X_train, y_train)\n",
    "    \n",
    "#     # Run the classifier on the test set\n",
    "#     y_pred = clf.predict(X_test)\n",
    "    \n",
    "#     # Compute the confusion matrix for the test predictions\n",
    "#     c = confusion_matrix(y_test, y_pred, labels=class_names)\n",
    "    \n",
    "#     # Aggregate this confusion matrix with the ones from previous\n",
    "#     # folds.\n",
    "#     cm += c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a describtion of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "\n",
    "Your write-up goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **Code Description** - By running the 'Evaluate()' function, the pulse rate from a ppg signal and from an accelerometer with x,y and z dimensions will be estimated using the 'RunPulseRateAlgorithm'. It returns an aggregated error on all subjects of troika dataset. The pulse for all subjects ranges from 60 to 180 BPM. More specifically, the 'RunPulseRateAlgorithm' takes the magnitude of the accelerations in x y and z axis, applies a bandpass filter to them and to the PPG signal, and for each window of size 8 sec (next window after 2 sec), gets the fourier transform of the magnitude of acceleration and of PPG signal, finds the peak values, compares them and if they are different they keep that dominant frequency and if not they calculate the next 4 until it finds the dominant. At the end, errors for each dominant frequency along with its confidence interval.\n",
    "    \n",
    "> - **Data Description** - Two-channel PPG signals, three-axis acceleration signals, and one-channel ECG signals were\n",
    "simultaneously recorded from subjects, aged 18-35. All signals were sampled at 125 Hz. Each dataset with the similar name 'DATA_01_TYPE01' contains a variable 'sig'. It has 6 rows. The first row is a simultaneous recording of ECG, which is recorded from the chest of each subject. The second row and the third row are two channels of PPG, which are recorded from the wrist of each subject. The last three rows are simultaneous recordings of acceleration data (in x-, y-, and z-axis). We also provide the calculated ground-truth heart rate, stored in the datasets with the corresponding name, say 'REF_01_TYPE01'. In each of this kind of datasets, there is a variable 'BPM0', which gives the BPM value in every 8-second time window. Note that two successive time windows overlap by 6 seconds. Thus the first value in 'BPM0' gives the calcualted heart rate ground-truth in the first 8 seconds, while the second value in 'BPM0' gives the calculated heart rate ground-truth from the 3rd second to the 10th second. There were 12 subjects in total for this study. Data from different studies should be incorporated if we want to have a more generalizable algorithm.\n",
    "\n",
    "> - **Algorithhm Description** - The algorithm estimates the BPM for each subject based on accelerometer and PPG data. It first applies a BandPass filter, and then, for each window of 8 seconds (which has an overlap of 6 secs with the previous window) it gets data corresponding to that time, transform them using Fourier, finding the peaks of both accelerometer (square root of sum for each axis) and PPG signal, sort them and compares them to see if they they have different peaks for each dominant frequency. If so, then they consist the pulse estimate. If not, then, for the next 4 dominant frequencies we check the same to find the estimate of the pulse. At the end, a confidence interval and the error of each prediction are estimated. It is worth noticing that algorithm is affected by motion of arms and of the presence of noise. In terms of physiology, when the ventricles contract, the capillaries in the wrist fill with blood. The (typically green) light emitted by the PPG sensor is absorbed by red blood cells in these capillaries and the photodetector will see the drop in reflected light. When the blood returns to the heart, fewer red blood cells in the wrist absorb the light and the photodetector sees an increase in reflected light. The period of this oscillating waveform is the pulse rate.\n",
    "\n",
    "> - **Algorithm Performance** - Mean Absolute Error (MAE) is used as a performance metric. By running the 'Evaluate()' function we can see that the total error for all subjects is 27.35. For the test subject, the error is 7.28. The algorithm can generalize to data outside of the data used but we should ensure that there is not much noise to them. More specifically, mean absolute error takes the absolute value of the predicted BPM minues the ground truth BPM. Predicted BMP are the estimates that are calculated by running the above algorithm using PPG and accelerometer data. The lower the value the closest the predictions to the ground truth. A mean absolute error of 7.28 for the test subject means that in total, for all the windows of 8 sec (with an overlap of 2 sec) the mean absolute difference of the predictions from the ground truth labels are no more that 7.28 BMP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** (back in the Project Classroom) to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
